{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-10T17:16:48.243096Z",
     "start_time": "2025-02-10T17:16:06.441445Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sampler.spray_sampler import SpraySampler\n",
    "\n",
    "directory = \"res/sprays\"\n",
    "spray_sampler = SpraySampler(directory)\n",
    "x, y = spray_sampler.sample(16_000)\n",
    "\n",
    "k = 4\n",
    "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.15, random_state=410)\n",
    "X_train = torch.stack([\n",
    "    torch.stack(\n",
    "        [torch.from_numpy(a)[1:k + 1, :], torch.from_numpy(b)[1:k + 1, :]]\n",
    "    ).flatten(start_dim=0)\n",
    "    for (a, b) in X_train\n",
    "])\n",
    "X_test = torch.stack([\n",
    "    torch.stack(\n",
    "        [torch.from_numpy(a)[1:k + 1, :], torch.from_numpy(b)[1:k + 1, :]]\n",
    "    ).flatten(start_dim=0)\n",
    "    for (a, b) in X_test\n",
    "])\n",
    "\n",
    "print(X_train.shape, X_test.shape)\n",
    "\n",
    "y_train = torch.tensor(y_train).float()\n",
    "y_test = torch.tensor(y_test).float()\n",
    "\n",
    "print(y_train.shape, y_test.shape)\n",
    "\n",
    "# print(len(X_train), type(X_train[0][0]), y_train.shape)\n",
    "# print(len(X_test), type(X_test[0][0]), y_test.shape)"
   ],
   "id": "2f9862ba6a0445a2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([13600, 32]) torch.Size([2400, 32])\n",
      "torch.Size([13600]) torch.Size([2400])\n"
     ]
    }
   ],
   "execution_count": 125
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-10T17:16:51.609285Z",
     "start_time": "2025-02-10T17:16:51.604498Z"
    }
   },
   "cell_type": "code",
   "source": [
    "mean, std = torch.mean(X_train, 0, keepdim=True), torch.std(X_train, 0, keepdim=True)\n",
    "\n",
    "X_train = (X_train - mean) / std\n",
    "X_test = (X_test - mean) / std"
   ],
   "id": "2e186bb12f8db86f",
   "outputs": [],
   "execution_count": 126
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-10T16:44:36.162083Z",
     "start_time": "2025-02-10T16:44:36.150424Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence\n",
    "#\n",
    "#\n",
    "# def pack_batch(batch: list):\n",
    "#     # Extract sequences and lengths\n",
    "#     # print(batch)\n",
    "#     seq1, seq2 = zip(*batch)\n",
    "#     seq1_lens = torch.tensor([len(s) for s in seq1])\n",
    "#     seq2_lens = torch.tensor([len(s) for s in seq2])\n",
    "#\n",
    "#     # Sort sequences by decreasing length for PackedSequence\n",
    "#     sorted_indices = torch.argsort(-seq1_lens)\n",
    "#     seq1_sorted = [seq1[i] for i in sorted_indices]\n",
    "#     seq1_lens_sorted = seq1_lens[sorted_indices]\n",
    "#     sorted_indices = torch.argsort(-seq2_lens)\n",
    "#     seq2_sorted = [seq2[i] for i in sorted_indices]\n",
    "#     seq2_lens_sorted = seq2_lens[sorted_indices]\n",
    "#\n",
    "#     # Pad sequences to maximum length in batch\n",
    "#     seq1_padded = pad_sequence(seq1_sorted, batch_first=True)\n",
    "#     seq2_padded = pad_sequence(seq2_sorted, batch_first=True)\n",
    "#\n",
    "#     # Pack sequences\n",
    "#     seq1_packed = pack_padded_sequence(seq1_padded, seq1_lens_sorted, batch_first=True, enforce_sorted=True)\n",
    "#     seq2_packed = pack_padded_sequence(seq2_padded, seq2_lens_sorted, batch_first=True, enforce_sorted=True)\n",
    "#     return seq1_packed, seq2_packed\n",
    "#\n",
    "#\n",
    "# class SprayIdentifier(torch.nn.Module):\n",
    "#     def __init__(self, hidden_size=16, num_layers=2):\n",
    "#         super(SprayIdentifier, self).__init__()\n",
    "#         self.lstm = torch.nn.LSTM(input_size=4, hidden_size=hidden_size, num_layers=num_layers, batch_first=True)\n",
    "#         self.layer_norm = torch.nn.LayerNorm(hidden_size * 4)\n",
    "#         self.linear = torch.nn.Linear(hidden_size * 4, 1)\n",
    "#\n",
    "#     def forward(self, batch):\n",
    "#         seq1_padded, seq2_padded = pack_batch(batch)\n",
    "#         _, (h1, c1) = self.lstm(seq1_padded)\n",
    "#         _, (h2, c2) = self.lstm(seq2_padded)\n",
    "#         latent = torch.cat((h1[-1], c1[-1], h2[-1], c2[-1]), dim=1)\n",
    "#         latent = self.layer_norm(latent)\n",
    "#         logits = self.linear(latent)\n",
    "#         return logits\n",
    "#\n",
    "#\n",
    "# SprayIdentifier()(X_train[0:5])"
   ],
   "id": "450c27a25dfcf8de",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.7732],\n",
       "        [-0.7127],\n",
       "        [-0.7614],\n",
       "        [-0.6672],\n",
       "        [-0.7464]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 63
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-10T17:18:34.100426Z",
     "start_time": "2025-02-10T17:18:34.097107Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class SprayIdentifier(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SprayIdentifier, self).__init__()\n",
    "        self.sequential = torch.nn.Sequential(\n",
    "            torch.nn.Linear(32, 16),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(16, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.sequential(x)"
   ],
   "id": "9d4116bdc49a4856",
   "outputs": [],
   "execution_count": 133
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-10T17:17:10.633083Z",
     "start_time": "2025-02-10T17:17:10.628940Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def train(model, loss_fn, optimizer, dataloader):\n",
    "    model.train()\n",
    "    for x_batch, y_batch in dataloader:\n",
    "        logits = model(x_batch).squeeze()\n",
    "        loss = loss_fn(logits, y_batch)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print(loss.item())\n",
    "\n",
    "\n",
    "def test(model, loss_fn, dataloader):\n",
    "    model.eval()\n",
    "    loss = 0\n",
    "    for x_batch, y_batch in dataloader:\n",
    "        logits = model(x_batch).squeeze()\n",
    "        loss += loss_fn(logits, y_batch).item()\n",
    "    return loss"
   ],
   "id": "69aacb0fc25bdbf2",
   "outputs": [],
   "execution_count": 131
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-10T17:19:19.819979Z",
     "start_time": "2025-02-10T17:18:36.043291Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model = SprayIdentifier()\n",
    "loss_fn = torch.nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "train_dataloader = DataLoader(train_dataset, shuffle=True, batch_size=32)\n",
    "\n",
    "test_dataset = TensorDataset(X_test, y_test)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=32)\n",
    "\n",
    "for _ in range(1_000):\n",
    "    train(model, loss_fn, optimizer, train_dataloader)\n",
    "    train_loss = test(model, loss_fn, train_dataloader) / len(train_dataloader)\n",
    "    test_loss = test(model, loss_fn, test_dataloader) / len(test_dataloader)\n",
    "    print(f\"loss:\\n\\t• {train_loss}\\n\\t• {test_loss}\")\n",
    "\n",
    "# loss:\n",
    "# \t• 185.07529932260513\n",
    "# \t• 32.755794167518616"
   ],
   "id": "abcc89072e19e415",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss:\n",
      "\t• 0.652421766168931\n",
      "\t• 0.6568835020065308\n",
      "loss:\n",
      "\t• 0.630175303290872\n",
      "\t• 0.6350029611587524\n",
      "loss:\n",
      "\t• 0.6208315590549918\n",
      "\t• 0.6269623359044393\n",
      "loss:\n",
      "\t• 0.613670035460416\n",
      "\t• 0.6233611714839935\n",
      "loss:\n",
      "\t• 0.6079946196079254\n",
      "\t• 0.6181556499004364\n",
      "loss:\n",
      "\t• 0.6041756175546085\n",
      "\t• 0.6139470521608988\n",
      "loss:\n",
      "\t• 0.5984271257765152\n",
      "\t• 0.6114702792962392\n",
      "loss:\n",
      "\t• 0.5926085426526911\n",
      "\t• 0.607778757015864\n",
      "loss:\n",
      "\t• 0.5876427991249982\n",
      "\t• 0.602424799601237\n",
      "loss:\n",
      "\t• 0.5839498611057505\n",
      "\t• 0.5986624081929525\n",
      "loss:\n",
      "\t• 0.5796510085638832\n",
      "\t• 0.5969138276576996\n",
      "loss:\n",
      "\t• 0.576385005642386\n",
      "\t• 0.5930312168598175\n",
      "loss:\n",
      "\t• 0.5756239154058345\n",
      "\t• 0.5935049184163411\n",
      "loss:\n",
      "\t• 0.5728106418777915\n",
      "\t• 0.5909797290960948\n",
      "loss:\n",
      "\t• 0.5712985470715691\n",
      "\t• 0.5922882334391276\n",
      "loss:\n",
      "\t• 0.5699510462845073\n",
      "\t• 0.5906130695343017\n",
      "loss:\n",
      "\t• 0.5684575641155243\n",
      "\t• 0.588916175365448\n",
      "loss:\n",
      "\t• 0.5687102479794446\n",
      "\t• 0.5942748661835988\n",
      "loss:\n",
      "\t• 0.5673243324195637\n",
      "\t• 0.5899820844332377\n",
      "loss:\n",
      "\t• 0.5652247997592478\n",
      "\t• 0.5903098714351654\n",
      "loss:\n",
      "\t• 0.5646802927466\n",
      "\t• 0.5897717718283335\n",
      "loss:\n",
      "\t• 0.5643793092054479\n",
      "\t• 0.591856821378072\n",
      "loss:\n",
      "\t• 0.5630206846489626\n",
      "\t• 0.5891894837220509\n",
      "loss:\n",
      "\t• 0.563511092522565\n",
      "\t• 0.5911773725350697\n",
      "loss:\n",
      "\t• 0.5637991471150342\n",
      "\t• 0.5930506507555644\n",
      "loss:\n",
      "\t• 0.5635089848322027\n",
      "\t• 0.5875923526287079\n",
      "loss:\n",
      "\t• 0.5606875697304221\n",
      "\t• 0.5885486229260762\n",
      "loss:\n",
      "\t• 0.5622770054200116\n",
      "\t• 0.5911631731192271\n",
      "loss:\n",
      "\t• 0.5602095944039962\n",
      "\t• 0.5880816435813904\n",
      "loss:\n",
      "\t• 0.5597246095012216\n",
      "\t• 0.5879097032546997\n",
      "loss:\n",
      "\t• 0.5598873144738814\n",
      "\t• 0.5914346774419149\n",
      "loss:\n",
      "\t• 0.5585052781245288\n",
      "\t• 0.5868603277206421\n",
      "loss:\n",
      "\t• 0.5577670606444863\n",
      "\t• 0.5874531710147858\n",
      "loss:\n",
      "\t• 0.5587002821529613\n",
      "\t• 0.5875364013512929\n",
      "loss:\n",
      "\t• 0.5568466012618121\n",
      "\t• 0.5871334699789683\n",
      "loss:\n",
      "\t• 0.5575285864577574\n",
      "\t• 0.5887624748547872\n",
      "loss:\n",
      "\t• 0.5558541646424462\n",
      "\t• 0.5908786149819693\n",
      "loss:\n",
      "\t• 0.5537816696307238\n",
      "\t• 0.588073566754659\n",
      "loss:\n",
      "\t• 0.5534962168160606\n",
      "\t• 0.5875860436757405\n",
      "loss:\n",
      "\t• 0.55428358056966\n",
      "\t• 0.5842763682206472\n",
      "loss:\n",
      "\t• 0.552461749385385\n",
      "\t• 0.5872364056110382\n",
      "loss:\n",
      "\t• 0.5526327473977033\n",
      "\t• 0.5886023286978403\n",
      "loss:\n",
      "\t• 0.5521714406153735\n",
      "\t• 0.5874671220779419\n",
      "loss:\n",
      "\t• 0.5502965362632976\n",
      "\t• 0.5862660217285156\n",
      "loss:\n",
      "\t• 0.5497515125835644\n",
      "\t• 0.5880502355098725\n",
      "loss:\n",
      "\t• 0.5493496443243587\n",
      "\t• 0.586224905649821\n",
      "loss:\n",
      "\t• 0.5501509073902578\n",
      "\t• 0.584250104824702\n",
      "loss:\n",
      "\t• 0.5492158919923446\n",
      "\t• 0.5879357628027598\n",
      "loss:\n",
      "\t• 0.5493294656977934\n",
      "\t• 0.5856356577078501\n",
      "loss:\n",
      "\t• 0.5491002244107863\n",
      "\t• 0.5899571593602498\n",
      "loss:\n",
      "\t• 0.546656820563709\n",
      "\t• 0.5856662734349569\n",
      "loss:\n",
      "\t• 0.5478532866169424\n",
      "\t• 0.5863302993774414\n",
      "loss:\n",
      "\t• 0.5476923865430495\n",
      "\t• 0.5865901295344035\n",
      "loss:\n",
      "\t• 0.5469396171149086\n",
      "\t• 0.5869008322556813\n",
      "loss:\n",
      "\t• 0.5480567232300253\n",
      "\t• 0.5850761648019155\n",
      "loss:\n",
      "\t• 0.5461876656728633\n",
      "\t• 0.5864508306980133\n",
      "loss:\n",
      "\t• 0.546466341790031\n",
      "\t• 0.584778638680776\n",
      "loss:\n",
      "\t• 0.5456620657444\n",
      "\t• 0.5848531397183736\n",
      "loss:\n",
      "\t• 0.5450012920183294\n",
      "\t• 0.5835960698127747\n",
      "loss:\n",
      "\t• 0.5444390820755678\n",
      "\t• 0.58361745595932\n",
      "loss:\n",
      "\t• 0.5438508893461789\n",
      "\t• 0.5832266096274058\n",
      "loss:\n",
      "\t• 0.5440665824974285\n",
      "\t• 0.5850144159793854\n",
      "loss:\n",
      "\t• 0.5432662144128014\n",
      "\t• 0.5837706502278646\n",
      "loss:\n",
      "\t• 0.5435179528068094\n",
      "\t• 0.583755320707957\n",
      "loss:\n",
      "\t• 0.5432478925761055\n",
      "\t• 0.5866635131835938\n",
      "loss:\n",
      "\t• 0.5434034823670106\n",
      "\t• 0.5807137684027354\n",
      "loss:\n",
      "\t• 0.5420398070531733\n",
      "\t• 0.5830069625377655\n",
      "loss:\n",
      "\t• 0.5424144742769353\n",
      "\t• 0.5859987099965414\n",
      "loss:\n",
      "\t• 0.5425854527249055\n",
      "\t• 0.5827037584781647\n",
      "loss:\n",
      "\t• 0.5416734151279226\n",
      "\t• 0.5854787421226502\n",
      "loss:\n",
      "\t• 0.5414491262856652\n",
      "\t• 0.58239275376002\n",
      "loss:\n",
      "\t• 0.5414953092266531\n",
      "\t• 0.58335582613945\n",
      "loss:\n",
      "\t• 0.5413284831187304\n",
      "\t• 0.5847727680206298\n",
      "loss:\n",
      "\t• 0.5418259711826549\n",
      "\t• 0.5854161457220713\n",
      "loss:\n",
      "\t• 0.5406647159071529\n",
      "\t• 0.5839719398816426\n",
      "loss:\n",
      "\t• 0.5398556372698615\n",
      "\t• 0.5841971528530121\n",
      "loss:\n",
      "\t• 0.5410936310712029\n",
      "\t• 0.5854087932904561\n",
      "loss:\n",
      "\t• 0.5403224285911111\n",
      "\t• 0.582203691403071\n",
      "loss:\n",
      "\t• 0.5404553806781769\n",
      "\t• 0.5866576345761617\n",
      "loss:\n",
      "\t• 0.5392280176106621\n",
      "\t• 0.583407742579778\n",
      "loss:\n",
      "\t• 0.5389621604190153\n",
      "\t• 0.5838875687122345\n",
      "loss:\n",
      "\t• 0.5407759345980252\n",
      "\t• 0.5872245093186697\n",
      "loss:\n",
      "\t• 0.5403870177970213\n",
      "\t• 0.585688728094101\n",
      "loss:\n",
      "\t• 0.5392041530328638\n",
      "\t• 0.5832283532619477\n",
      "loss:\n",
      "\t• 0.5385631137735704\n",
      "\t• 0.580795415242513\n",
      "loss:\n",
      "\t• 0.5385533846827114\n",
      "\t• 0.5850674319267273\n",
      "loss:\n",
      "\t• 0.5368375633744632\n",
      "\t• 0.5827277608712514\n",
      "loss:\n",
      "\t• 0.5384151666304644\n",
      "\t• 0.5862064107259115\n",
      "loss:\n",
      "\t• 0.5367866859716528\n",
      "\t• 0.5831238881746927\n",
      "loss:\n",
      "\t• 0.5385302240708295\n",
      "\t• 0.5853654523690541\n",
      "loss:\n",
      "\t• 0.53666803577367\n",
      "\t• 0.583259494304657\n",
      "loss:\n",
      "\t• 0.5394133011733785\n",
      "\t• 0.5873059002558391\n",
      "loss:\n",
      "\t• 0.535871795205509\n",
      "\t• 0.581300931374232\n",
      "loss:\n",
      "\t• 0.5370528473573573\n",
      "\t• 0.5855742128690083\n",
      "loss:\n",
      "\t• 0.5366953098773957\n",
      "\t• 0.5831093394756317\n",
      "loss:\n",
      "\t• 0.534766578463947\n",
      "\t• 0.5825396366914113\n",
      "loss:\n",
      "\t• 0.5370564593988306\n",
      "\t• 0.5881205014387767\n",
      "loss:\n",
      "\t• 0.5351328661161311\n",
      "\t• 0.5844165019194285\n",
      "loss:\n",
      "\t• 0.5354332846052506\n",
      "\t• 0.5878091339270274\n",
      "loss:\n",
      "\t• 0.5336555613489712\n",
      "\t• 0.5827340086301168\n",
      "loss:\n",
      "\t• 0.5344797523582683\n",
      "\t• 0.5814774437745412\n",
      "loss:\n",
      "\t• 0.5331919541078455\n",
      "\t• 0.5827799260616302\n",
      "loss:\n",
      "\t• 0.5337053055622999\n",
      "\t• 0.5837804587682088\n",
      "loss:\n",
      "\t• 0.5337467942518347\n",
      "\t• 0.5815387443701426\n",
      "loss:\n",
      "\t• 0.5352313320075764\n",
      "\t• 0.5828386386235554\n",
      "loss:\n",
      "\t• 0.5337468227919411\n",
      "\t• 0.5855959244569142\n",
      "loss:\n",
      "\t• 0.5332688334408928\n",
      "\t• 0.5814190951983134\n",
      "loss:\n",
      "\t• 0.5320211059205673\n",
      "\t• 0.5824465692043305\n",
      "loss:\n",
      "\t• 0.5322155187410467\n",
      "\t• 0.5827663977940877\n",
      "loss:\n",
      "\t• 0.5351104869562037\n",
      "\t• 0.5861311984062195\n",
      "loss:\n",
      "\t• 0.5327121757058536\n",
      "\t• 0.5854268542925517\n",
      "loss:\n",
      "\t• 0.5322070353872635\n",
      "\t• 0.5820885169506073\n",
      "loss:\n",
      "\t• 0.5306682815972497\n",
      "\t• 0.5806042238076528\n",
      "loss:\n",
      "\t• 0.5347409648053786\n",
      "\t• 0.5822180728117625\n",
      "loss:\n",
      "\t• 0.5308730347016278\n",
      "\t• 0.5813094433148702\n",
      "loss:\n",
      "\t• 0.5313128005757052\n",
      "\t• 0.5805182055632273\n",
      "loss:\n",
      "\t• 0.5300030434832853\n",
      "\t• 0.5805493334929148\n",
      "loss:\n",
      "\t• 0.5306924859916463\n",
      "\t• 0.5822075351079306\n",
      "loss:\n",
      "\t• 0.5294963436266955\n",
      "\t• 0.5788584991296133\n",
      "loss:\n",
      "\t• 0.5311382879930384\n",
      "\t• 0.5844778700669606\n",
      "loss:\n",
      "\t• 0.5312601734610165\n",
      "\t• 0.5842419834931691\n",
      "loss:\n",
      "\t• 0.5292570990674635\n",
      "\t• 0.5808030784130096\n",
      "loss:\n",
      "\t• 0.5297252223772161\n",
      "\t• 0.5812627720832825\n",
      "loss:\n",
      "\t• 0.5300088716254515\n",
      "\t• 0.5801362633705139\n",
      "loss:\n",
      "\t• 0.5296441876187044\n",
      "\t• 0.5813911366462707\n",
      "loss:\n",
      "\t• 0.5299689223485834\n",
      "\t• 0.5813793802261352\n",
      "loss:\n",
      "\t• 0.529797602260814\n",
      "\t• 0.585992286602656\n",
      "loss:\n",
      "\t• 0.5290009823266197\n",
      "\t• 0.5836080626646678\n",
      "loss:\n",
      "\t• 0.5282396546532127\n",
      "\t• 0.5779584062099457\n",
      "loss:\n",
      "\t• 0.5298457728413974\n",
      "\t• 0.5798118325074514\n",
      "loss:\n",
      "\t• 0.5298950238087597\n",
      "\t• 0.5841415210564931\n",
      "loss:\n",
      "\t• 0.5284627194965587\n",
      "\t• 0.5793708145618439\n",
      "loss:\n",
      "\t• 0.529129516657661\n",
      "\t• 0.585005186398824\n",
      "loss:\n",
      "\t• 0.528897635866614\n",
      "\t• 0.5782437936464946\n",
      "loss:\n",
      "\t• 0.5268884364997639\n",
      "\t• 0.5799249712626139\n",
      "loss:\n",
      "\t• 0.5276038194403929\n",
      "\t• 0.5826287345091502\n",
      "loss:\n",
      "\t• 0.5300297398426953\n",
      "\t• 0.5834727680683136\n",
      "loss:\n",
      "\t• 0.5271493809363421\n",
      "\t• 0.5795110114415487\n",
      "loss:\n",
      "\t• 0.5283088455480688\n",
      "\t• 0.5862593034903208\n",
      "loss:\n",
      "\t• 0.5266527323161855\n",
      "\t• 0.5796342631181081\n",
      "loss:\n",
      "\t• 0.5264922397978166\n",
      "\t• 0.5785704255104065\n",
      "loss:\n",
      "\t• 0.5278751104018268\n",
      "\t• 0.5819187311331431\n",
      "loss:\n",
      "\t• 0.527940565067179\n",
      "\t• 0.5830331615606944\n",
      "loss:\n",
      "\t• 0.5262690320435692\n",
      "\t• 0.5810140407085419\n",
      "loss:\n",
      "\t• 0.5278820511172799\n",
      "\t• 0.5844026569525401\n",
      "loss:\n",
      "\t• 0.5255525196299834\n",
      "\t• 0.5800189658006032\n",
      "loss:\n",
      "\t• 0.5256649677192463\n",
      "\t• 0.580364590883255\n",
      "loss:\n",
      "\t• 0.5253556858090793\n",
      "\t• 0.5783837322394053\n",
      "loss:\n",
      "\t• 0.5261525819582098\n",
      "\t• 0.5831668456395467\n",
      "loss:\n",
      "\t• 0.5250254095301908\n",
      "\t• 0.5796870644887289\n",
      "loss:\n",
      "\t• 0.5252160098973443\n",
      "\t• 0.5798857927322387\n",
      "loss:\n",
      "\t• 0.5289973820658291\n",
      "\t• 0.5807886401812236\n",
      "loss:\n",
      "\t• 0.5270131357277141\n",
      "\t• 0.5827052223682404\n",
      "loss:\n",
      "\t• 0.5257762434903314\n",
      "\t• 0.5819631671905517\n",
      "loss:\n",
      "\t• 0.5280935518180623\n",
      "\t• 0.5816316743691762\n",
      "loss:\n",
      "\t• 0.5253355019232806\n",
      "\t• 0.5788378822803497\n",
      "loss:\n",
      "\t• 0.5267658395626965\n",
      "\t• 0.5799626131852468\n",
      "loss:\n",
      "\t• 0.5255912621582256\n",
      "\t• 0.5797888068358104\n",
      "loss:\n",
      "\t• 0.526197890253628\n",
      "\t• 0.5822782568136851\n",
      "loss:\n",
      "\t• 0.5238852419572718\n",
      "\t• 0.5774342131614685\n",
      "loss:\n",
      "\t• 0.5252419031367582\n",
      "\t• 0.5780533345540365\n",
      "loss:\n",
      "\t• 0.5248881888389587\n",
      "\t• 0.581891440153122\n",
      "loss:\n",
      "\t• 0.5237094186334049\n",
      "\t• 0.5767999895413717\n",
      "loss:\n",
      "\t• 0.5248664304088144\n",
      "\t• 0.5815817272663116\n",
      "loss:\n",
      "\t• 0.5245669165078332\n",
      "\t• 0.5804364720980326\n",
      "loss:\n",
      "\t• 0.525897760811974\n",
      "\t• 0.5850833904743195\n",
      "loss:\n",
      "\t• 0.5241034084909102\n",
      "\t• 0.5788029166062673\n",
      "loss:\n",
      "\t• 0.5226941201266121\n",
      "\t• 0.5795241947968801\n",
      "loss:\n",
      "\t• 0.5255333256020266\n",
      "\t• 0.5798240840435028\n",
      "loss:\n",
      "\t• 0.523649778436212\n",
      "\t• 0.5778786754608154\n",
      "loss:\n",
      "\t• 0.522829686052659\n",
      "\t• 0.5803519372145335\n",
      "loss:\n",
      "\t• 0.5232917442742516\n",
      "\t• 0.5777517183621724\n",
      "loss:\n",
      "\t• 0.5232850549501531\n",
      "\t• 0.5783916048208873\n",
      "loss:\n",
      "\t• 0.522886356045218\n",
      "\t• 0.5786278986930847\n",
      "loss:\n",
      "\t• 0.5239521228565889\n",
      "\t• 0.5782241602738698\n",
      "loss:\n",
      "\t• 0.5231846852863536\n",
      "\t• 0.5759206215540568\n",
      "loss:\n",
      "\t• 0.5238869852879469\n",
      "\t• 0.5784762104352316\n",
      "loss:\n",
      "\t• 0.5239002194825341\n",
      "\t• 0.5785332453250885\n",
      "loss:\n",
      "\t• 0.522905224035768\n",
      "\t• 0.5792792093753815\n",
      "loss:\n",
      "\t• 0.5236814436491798\n",
      "\t• 0.5801579872767131\n",
      "loss:\n",
      "\t• 0.5227251716922311\n",
      "\t• 0.577833182811737\n",
      "loss:\n",
      "\t• 0.5218972351971795\n",
      "\t• 0.5756288015842438\n",
      "loss:\n",
      "\t• 0.5233936451463138\n",
      "\t• 0.5806815254688263\n",
      "loss:\n",
      "\t• 0.5234896144446205\n",
      "\t• 0.5776199849446615\n",
      "loss:\n",
      "\t• 0.5219257521629334\n",
      "\t• 0.5767701160907746\n",
      "loss:\n",
      "\t• 0.5223324375994065\n",
      "\t• 0.5775514348347982\n",
      "loss:\n",
      "\t• 0.5212287872679093\n",
      "\t• 0.5763369139035542\n",
      "loss:\n",
      "\t• 0.5237542207802043\n",
      "\t• 0.5823210004965464\n",
      "loss:\n",
      "\t• 0.5222441106684068\n",
      "\t• 0.5779071152210236\n",
      "loss:\n",
      "\t• 0.5222051977410036\n",
      "\t• 0.5764265326658885\n",
      "loss:\n",
      "\t• 0.5221399974822998\n",
      "\t• 0.5792136772473653\n",
      "loss:\n",
      "\t• 0.521442927683101\n",
      "\t• 0.5761619253953298\n",
      "loss:\n",
      "\t• 0.5222729786704569\n",
      "\t• 0.5803382635116577\n",
      "loss:\n",
      "\t• 0.5216891023691963\n",
      "\t• 0.5787215852737426\n",
      "loss:\n",
      "\t• 0.5251405640910654\n",
      "\t• 0.5825057649612426\n",
      "loss:\n",
      "\t• 0.5211184454665464\n",
      "\t• 0.5777719326814016\n",
      "loss:\n",
      "\t• 0.5219235009305617\n",
      "\t• 0.5764959955215454\n",
      "loss:\n",
      "\t• 0.5218979703678804\n",
      "\t• 0.5795658536752065\n",
      "loss:\n",
      "\t• 0.5215792747806101\n",
      "\t• 0.5766901747385661\n",
      "loss:\n",
      "\t• 0.5200980844217188\n",
      "\t• 0.5773424812157949\n",
      "loss:\n",
      "\t• 0.5234310945342568\n",
      "\t• 0.5771285541852316\n",
      "loss:\n",
      "\t• 0.5213924729122835\n",
      "\t• 0.5783247303962707\n",
      "loss:\n",
      "\t• 0.5216179721495684\n",
      "\t• 0.5816127673784892\n",
      "loss:\n",
      "\t• 0.5226867506784552\n",
      "\t• 0.578922199010849\n",
      "loss:\n",
      "\t• 0.5205325009542353\n",
      "\t• 0.5791648888587951\n",
      "loss:\n",
      "\t• 0.5207385993705076\n",
      "\t• 0.578323768377304\n",
      "loss:\n",
      "\t• 0.5219710691536175\n",
      "\t• 0.5816227058569591\n",
      "loss:\n",
      "\t• 0.5207138925440171\n",
      "\t• 0.5773852666219076\n",
      "loss:\n",
      "\t• 0.5204696999577915\n",
      "\t• 0.577905269463857\n",
      "loss:\n",
      "\t• 0.5213942107032327\n",
      "\t• 0.5800090344746908\n",
      "loss:\n",
      "\t• 0.5230660622961381\n",
      "\t• 0.5797724223136902\n",
      "loss:\n",
      "\t• 0.520709961793002\n",
      "\t• 0.5786986486117045\n",
      "loss:\n",
      "\t• 0.5213427648123573\n",
      "\t• 0.5790076243877411\n",
      "loss:\n",
      "\t• 0.5202068153549643\n",
      "\t• 0.5767650087674459\n",
      "loss:\n",
      "\t• 0.5216262219933903\n",
      "\t• 0.5795179565747579\n",
      "loss:\n",
      "\t• 0.5206836819648742\n",
      "\t• 0.5753929150104523\n",
      "loss:\n",
      "\t• 0.5204224944815916\n",
      "\t• 0.5784944852193197\n",
      "loss:\n",
      "\t• 0.5210714464327868\n",
      "\t• 0.5766964713732402\n",
      "loss:\n",
      "\t• 0.5199098061112797\n",
      "\t• 0.5757416661580403\n",
      "loss:\n",
      "\t• 0.5228318306979011\n",
      "\t• 0.5832395060857137\n",
      "loss:\n",
      "\t• 0.5194610487713534\n",
      "\t• 0.5792524846394856\n",
      "loss:\n",
      "\t• 0.5208652689176447\n",
      "\t• 0.581468765338262\n",
      "loss:\n",
      "\t• 0.521163636936861\n",
      "\t• 0.5822999596595764\n",
      "loss:\n",
      "\t• 0.5203013050556183\n",
      "\t• 0.5801033679644266\n",
      "loss:\n",
      "\t• 0.521363024150624\n",
      "\t• 0.5834895360469818\n",
      "loss:\n",
      "\t• 0.5198976166809306\n",
      "\t• 0.578733519713084\n",
      "loss:\n",
      "\t• 0.5198098965252147\n",
      "\t• 0.5781431631247202\n",
      "loss:\n",
      "\t• 0.5199978370526258\n",
      "\t• 0.5780085388819377\n",
      "loss:\n",
      "\t• 0.5213280688313877\n",
      "\t• 0.5784243524074555\n",
      "loss:\n",
      "\t• 0.5196286681820365\n",
      "\t• 0.58037899573644\n",
      "loss:\n",
      "\t• 0.5221367207695456\n",
      "\t• 0.5824471314748129\n",
      "loss:\n",
      "\t• 0.5214893275849959\n",
      "\t• 0.5782640747229258\n",
      "loss:\n",
      "\t• 0.5210378588648403\n",
      "\t• 0.5833967789014181\n",
      "loss:\n",
      "\t• 0.521652375459671\n",
      "\t• 0.5849273665746053\n",
      "loss:\n",
      "\t• 0.5195664626710556\n",
      "\t• 0.5803080451488495\n",
      "loss:\n",
      "\t• 0.5201103515484754\n",
      "\t• 0.5805756525198619\n",
      "loss:\n",
      "\t• 0.5202599979148191\n",
      "\t• 0.5794634143511455\n",
      "loss:\n",
      "\t• 0.5202104593024535\n",
      "\t• 0.581438945531845\n",
      "loss:\n",
      "\t• 0.5198422743292416\n",
      "\t• 0.5819835980733236\n",
      "loss:\n",
      "\t• 0.5214483256199781\n",
      "\t• 0.5802326055367788\n",
      "loss:\n",
      "\t• 0.5207311492807725\n",
      "\t• 0.5800011392434438\n",
      "loss:\n",
      "\t• 0.5200263711284189\n",
      "\t• 0.5825829120477041\n",
      "loss:\n",
      "\t• 0.5202138620965621\n",
      "\t• 0.5849708791573842\n",
      "loss:\n",
      "\t• 0.5213188856489518\n",
      "\t• 0.5854203351338705\n",
      "loss:\n",
      "\t• 0.5195204499889823\n",
      "\t• 0.576315084695816\n",
      "loss:\n",
      "\t• 0.5194235451081219\n",
      "\t• 0.579390789270401\n",
      "loss:\n",
      "\t• 0.5210173561292536\n",
      "\t• 0.5800479439894358\n",
      "loss:\n",
      "\t• 0.5203965061552385\n",
      "\t• 0.5783670544624329\n",
      "loss:\n",
      "\t• 0.5202827906608581\n",
      "\t• 0.5799557089805603\n",
      "loss:\n",
      "\t• 0.5190720708229962\n",
      "\t• 0.5778760413328806\n",
      "loss:\n",
      "\t• 0.5199328947768492\n",
      "\t• 0.5823178219795228\n",
      "loss:\n",
      "\t• 0.5194321628177867\n",
      "\t• 0.5804100322723389\n",
      "loss:\n",
      "\t• 0.5204309199136846\n",
      "\t• 0.5806679932276407\n",
      "loss:\n",
      "\t• 0.5202316129207611\n",
      "\t• 0.5827488434314728\n",
      "loss:\n",
      "\t• 0.5203633541219375\n",
      "\t• 0.579455719391505\n",
      "loss:\n",
      "\t• 0.5187490463958067\n",
      "\t• 0.5799868257840475\n",
      "loss:\n",
      "\t• 0.518717053006677\n",
      "\t• 0.57721067070961\n",
      "loss:\n",
      "\t• 0.5186316638834336\n",
      "\t• 0.5822407249609629\n",
      "loss:\n",
      "\t• 0.5222374768116895\n",
      "\t• 0.5848248525460561\n",
      "loss:\n",
      "\t• 0.5207007316280814\n",
      "\t• 0.581775023539861\n",
      "loss:\n",
      "\t• 0.5200258798458997\n",
      "\t• 0.5797104156017303\n",
      "loss:\n",
      "\t• 0.5201618213513318\n",
      "\t• 0.5790956695874532\n",
      "loss:\n",
      "\t• 0.5198559598361745\n",
      "\t• 0.5799658596515656\n",
      "loss:\n",
      "\t• 0.5188151144280153\n",
      "\t• 0.5800898476441702\n",
      "loss:\n",
      "\t• 0.52021267841844\n",
      "\t• 0.5823207052548727\n",
      "loss:\n",
      "\t• 0.518687839087318\n",
      "\t• 0.5814091114203135\n",
      "loss:\n",
      "\t• 0.5183503598325393\n",
      "\t• 0.5764388438065847\n",
      "loss:\n",
      "\t• 0.519185701678781\n",
      "\t• 0.5776730028788248\n",
      "loss:\n",
      "\t• 0.5184917569160461\n",
      "\t• 0.5788830324014028\n",
      "loss:\n",
      "\t• 0.5179125121060539\n",
      "\t• 0.5813813821474711\n",
      "loss:\n",
      "\t• 0.5195147531874039\n",
      "\t• 0.5819420528411865\n",
      "loss:\n",
      "\t• 0.5204045965391046\n",
      "\t• 0.5818529240290324\n",
      "loss:\n",
      "\t• 0.517977386713028\n",
      "\t• 0.5803464694817861\n",
      "loss:\n",
      "\t• 0.5204887594194973\n",
      "\t• 0.5792632059256235\n",
      "loss:\n",
      "\t• 0.5204167565177469\n",
      "\t• 0.5839420855045319\n",
      "loss:\n",
      "\t• 0.5195697526370778\n",
      "\t• 0.5807780961195628\n",
      "loss:\n",
      "\t• 0.5182394703696755\n",
      "\t• 0.5783100187778473\n",
      "loss:\n",
      "\t• 0.52144398170359\n",
      "\t• 0.5837858112653097\n",
      "loss:\n",
      "\t• 0.5202839890648338\n",
      "\t• 0.582108279466629\n",
      "loss:\n",
      "\t• 0.5181125272021574\n",
      "\t• 0.5820707492033641\n",
      "loss:\n",
      "\t• 0.5196085782612071\n",
      "\t• 0.5782118046283722\n",
      "loss:\n",
      "\t• 0.51817082194721\n",
      "\t• 0.5819489860534668\n",
      "loss:\n",
      "\t• 0.5186295391531551\n",
      "\t• 0.5834637037913004\n",
      "loss:\n",
      "\t• 0.5173876981875476\n",
      "\t• 0.5774700733025869\n",
      "loss:\n",
      "\t• 0.5175770268019508\n",
      "\t• 0.5807430171966552\n",
      "loss:\n",
      "\t• 0.5171191012859344\n",
      "\t• 0.5788732914129893\n",
      "loss:\n",
      "\t• 0.5200393352789038\n",
      "\t• 0.5835187876224518\n",
      "loss:\n",
      "\t• 0.5183662028172437\n",
      "\t• 0.5813683307170868\n",
      "loss:\n",
      "\t• 0.5190251205247991\n",
      "\t• 0.5787619729836782\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[134], line 14\u001B[0m\n\u001B[1;32m     11\u001B[0m test_dataloader \u001B[38;5;241m=\u001B[39m DataLoader(test_dataset, batch_size\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m32\u001B[39m)\n\u001B[1;32m     13\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m _ \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;241m1_000\u001B[39m):\n\u001B[0;32m---> 14\u001B[0m     \u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mloss_fn\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moptimizer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain_dataloader\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     15\u001B[0m     train_loss \u001B[38;5;241m=\u001B[39m test(model, loss_fn, train_dataloader) \u001B[38;5;241m/\u001B[39m \u001B[38;5;28mlen\u001B[39m(train_dataloader)\n\u001B[1;32m     16\u001B[0m     test_loss \u001B[38;5;241m=\u001B[39m test(model, loss_fn, test_dataloader) \u001B[38;5;241m/\u001B[39m \u001B[38;5;28mlen\u001B[39m(test_dataloader)\n",
      "Cell \u001B[0;32mIn[131], line 9\u001B[0m, in \u001B[0;36mtrain\u001B[0;34m(model, loss_fn, optimizer, dataloader)\u001B[0m\n\u001B[1;32m      7\u001B[0m optimizer\u001B[38;5;241m.\u001B[39mzero_grad()\n\u001B[1;32m      8\u001B[0m loss\u001B[38;5;241m.\u001B[39mbackward()\n\u001B[0;32m----> 9\u001B[0m \u001B[43moptimizer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstep\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/msai/msthesis/.venv/lib/python3.10/site-packages/torch/optim/optimizer.py:487\u001B[0m, in \u001B[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    482\u001B[0m         \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    483\u001B[0m             \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\n\u001B[1;32m    484\u001B[0m                 \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mfunc\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mresult\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    485\u001B[0m             )\n\u001B[0;32m--> 487\u001B[0m out \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    488\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_optimizer_step_code()\n\u001B[1;32m    490\u001B[0m \u001B[38;5;66;03m# call optimizer step post hooks\u001B[39;00m\n",
      "File \u001B[0;32m~/msai/msthesis/.venv/lib/python3.10/site-packages/torch/optim/optimizer.py:91\u001B[0m, in \u001B[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m     89\u001B[0m     torch\u001B[38;5;241m.\u001B[39mset_grad_enabled(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdefaults[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdifferentiable\u001B[39m\u001B[38;5;124m\"\u001B[39m])\n\u001B[1;32m     90\u001B[0m     torch\u001B[38;5;241m.\u001B[39m_dynamo\u001B[38;5;241m.\u001B[39mgraph_break()\n\u001B[0;32m---> 91\u001B[0m     ret \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     92\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[1;32m     93\u001B[0m     torch\u001B[38;5;241m.\u001B[39m_dynamo\u001B[38;5;241m.\u001B[39mgraph_break()\n",
      "File \u001B[0;32m~/msai/msthesis/.venv/lib/python3.10/site-packages/torch/optim/adam.py:197\u001B[0m, in \u001B[0;36mAdam.step\u001B[0;34m(self, closure)\u001B[0m\n\u001B[1;32m    189\u001B[0m \u001B[38;5;129m@_use_grad_for_differentiable\u001B[39m\n\u001B[1;32m    190\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mstep\u001B[39m(\u001B[38;5;28mself\u001B[39m, closure\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[1;32m    191\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Perform a single optimization step.\u001B[39;00m\n\u001B[1;32m    192\u001B[0m \n\u001B[1;32m    193\u001B[0m \u001B[38;5;124;03m    Args:\u001B[39;00m\n\u001B[1;32m    194\u001B[0m \u001B[38;5;124;03m        closure (Callable, optional): A closure that reevaluates the model\u001B[39;00m\n\u001B[1;32m    195\u001B[0m \u001B[38;5;124;03m            and returns the loss.\u001B[39;00m\n\u001B[1;32m    196\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m--> 197\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_cuda_graph_capture_health_check\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    199\u001B[0m     loss \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m    200\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m closure \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "File \u001B[0;32m~/msai/msthesis/.venv/lib/python3.10/site-packages/torch/optim/optimizer.py:427\u001B[0m, in \u001B[0;36mOptimizer._cuda_graph_capture_health_check\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    414\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21m_cuda_graph_capture_health_check\u001B[39m(\u001B[38;5;28mself\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    415\u001B[0m     \u001B[38;5;66;03m# Note [torch.compile x capturable]\u001B[39;00m\n\u001B[1;32m    416\u001B[0m     \u001B[38;5;66;03m# If we are compiling, we try to take the capturable path automatically by\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    423\u001B[0m     \u001B[38;5;66;03m# Thus, when compiling, inductor will determine if cudagraphs\u001B[39;00m\n\u001B[1;32m    424\u001B[0m     \u001B[38;5;66;03m# can be enabled based on whether there is input mutation or CPU tensors.\u001B[39;00m\n\u001B[1;32m    425\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m (\n\u001B[1;32m    426\u001B[0m         \u001B[38;5;129;01mnot\u001B[39;00m is_compiling()\n\u001B[0;32m--> 427\u001B[0m         \u001B[38;5;129;01mand\u001B[39;00m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackends\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcuda\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mis_built\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    428\u001B[0m         \u001B[38;5;129;01mand\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mcuda\u001B[38;5;241m.\u001B[39mis_available()\n\u001B[1;32m    429\u001B[0m     ):\n\u001B[1;32m    430\u001B[0m         capturing \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mcuda\u001B[38;5;241m.\u001B[39mis_current_stream_capturing()\n\u001B[1;32m    432\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m capturing \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mall\u001B[39m(\n\u001B[1;32m    433\u001B[0m             group[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcapturable\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;28;01mfor\u001B[39;00m group \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mparam_groups\n\u001B[1;32m    434\u001B[0m         ):\n",
      "File \u001B[0;32m~/msai/msthesis/.venv/lib/python3.10/site-packages/torch/backends/cuda/__init__.py:38\u001B[0m, in \u001B[0;36mis_built\u001B[0;34m()\u001B[0m\n\u001B[1;32m      6\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mtorch\u001B[39;00m\n\u001B[1;32m      9\u001B[0m __all__ \u001B[38;5;241m=\u001B[39m [\n\u001B[1;32m     10\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mis_built\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m     11\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcuFFTPlanCacheAttrContextProp\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     34\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msdp_kernel\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m     35\u001B[0m ]\n\u001B[0;32m---> 38\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mis_built\u001B[39m():\n\u001B[1;32m     39\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124mr\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m     40\u001B[0m \u001B[38;5;124;03m    Return whether PyTorch is built with CUDA support.\u001B[39;00m\n\u001B[1;32m     41\u001B[0m \n\u001B[1;32m     42\u001B[0m \u001B[38;5;124;03m    Note that this doesn't necessarily mean CUDA is available; just that if this PyTorch\u001B[39;00m\n\u001B[1;32m     43\u001B[0m \u001B[38;5;124;03m    binary were run on a machine with working CUDA drivers and devices, we would be able to use it.\u001B[39;00m\n\u001B[1;32m     44\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m     45\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m torch\u001B[38;5;241m.\u001B[39m_C\u001B[38;5;241m.\u001B[39m_has_cuda\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 134
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-10T17:04:30.670911Z",
     "start_time": "2025-02-10T17:04:30.584517Z"
    }
   },
   "cell_type": "code",
   "source": [
    "true, pred, scores = [], [], []\n",
    "\n",
    "model.eval()\n",
    "for x_batch, y_batch in train_dataloader:\n",
    "    logits = model(x_batch).squeeze()\n",
    "    prob = torch.sigmoid(logits)\n",
    "\n",
    "    true.extend(y_batch)\n",
    "    pred.append(1 if prob.item() > 0.5 else 0)\n",
    "    scores.append(prob.item())"
   ],
   "id": "84bf3d2851cf85bc",
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "linear(): argument 'input' (position 1) must be Tensor, not list",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[87], line 5\u001B[0m\n\u001B[1;32m      3\u001B[0m model\u001B[38;5;241m.\u001B[39meval()\n\u001B[1;32m      4\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m pair, y \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mzip\u001B[39m(X_train, y_train):\n\u001B[0;32m----> 5\u001B[0m     logits \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[43m[\u001B[49m\u001B[43mpair\u001B[49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39msqueeze()\n\u001B[1;32m      6\u001B[0m     prob \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39msigmoid(logits)\n\u001B[1;32m      8\u001B[0m     true\u001B[38;5;241m.\u001B[39mappend(y\u001B[38;5;241m.\u001B[39mitem())\n",
      "File \u001B[0;32m~/msai/msthesis/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1734\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1735\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1736\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/msai/msthesis/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1742\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1743\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1744\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1745\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1746\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1747\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1749\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   1750\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "Cell \u001B[0;32mIn[84], line 13\u001B[0m, in \u001B[0;36mSprayIdentifier.forward\u001B[0;34m(self, x)\u001B[0m\n\u001B[1;32m     12\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, x):\n\u001B[0;32m---> 13\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msequential\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/msai/msthesis/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1734\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1735\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1736\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/msai/msthesis/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1742\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1743\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1744\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1745\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1746\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1747\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1749\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   1750\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "File \u001B[0;32m~/msai/msthesis/.venv/lib/python3.10/site-packages/torch/nn/modules/container.py:250\u001B[0m, in \u001B[0;36mSequential.forward\u001B[0;34m(self, input)\u001B[0m\n\u001B[1;32m    248\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m):\n\u001B[1;32m    249\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m module \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m:\n\u001B[0;32m--> 250\u001B[0m         \u001B[38;5;28minput\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[43mmodule\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m    251\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28minput\u001B[39m\n",
      "File \u001B[0;32m~/msai/msthesis/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1734\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1735\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1736\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/msai/msthesis/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1742\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1743\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1744\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1745\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1746\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1747\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1749\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   1750\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "File \u001B[0;32m~/msai/msthesis/.venv/lib/python3.10/site-packages/torch/nn/modules/linear.py:125\u001B[0m, in \u001B[0;36mLinear.forward\u001B[0;34m(self, input)\u001B[0m\n\u001B[1;32m    124\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[0;32m--> 125\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlinear\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbias\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mTypeError\u001B[0m: linear(): argument 'input' (position 1) must be Tensor, not list"
     ]
    }
   ],
   "execution_count": 87
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-10T16:52:25.527583Z",
     "start_time": "2025-02-10T16:42:00.828496Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import sklearn\n",
    "\n",
    "cm = sklearn.metrics.confusion_matrix(true, pred)\n",
    "auc = sklearn.metrics.roc_auc_score(true, scores)\n",
    "print(cm, auc)"
   ],
   "id": "db9bd4c0026843ff",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[607 460]\n",
      " [461 597]] 0.585390818913513\n"
     ]
    }
   ],
   "execution_count": 57
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-10T16:52:25.527854Z",
     "start_time": "2025-02-10T16:42:07.121851Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "\n",
    "fpr, tpr, thresholds = sklearn.metrics.roc_curve(true, scores)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(np.arange(0, 2), np.arange(0, 2), linestyle=\"--\", color=\"gray\")\n",
    "plt.plot(fpr, tpr)\n",
    "plt.show()"
   ],
   "id": "ec56a23f27e1fb52",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 58
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
